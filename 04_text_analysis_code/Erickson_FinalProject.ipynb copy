{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979923b5-0493-4768-ad1a-06db54f0bc7a",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "\n",
    "DS 5001 Exploratory Text Analytics | Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046f57f-12ed-4259-be3d-60cb67b8d044",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "- Full Name: Steffen Erickson\n",
    "- Userid: cns8vg\n",
    "- GitHub Repo URL: https://github.com/steffenerickson/NSF_Code/tree/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/pl8uvwelm5ff8bhp1qvlj/AA0_9fUgMLZ28Y7oloo60dc?rlkey=6unk1pqig6mp4eorrediaxuoc&st=z1rtvsg7&dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acd11d-eb04-4bcc-b115-f205f367de49",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of the final project is for you to create a **digital analytical edition** of a corpus using the tools, practices, and perspectives youâ€™ve learning in this course. You will select a corpus that has already been digitized and transcribed, parse that into an F-compliant set of tables, and then generate and visualize the results of a series of fitted models. You will also draw some tentative conclusions regarding the linguistic, cultural, psychological, or historical features represented by your corpus. The point of the exercise is to have you work with a corpus through the entire pipeline from ingestion to interpretation. \n",
    "\n",
    "Specifically, you will acquire a collection of long-form texts and perform the following operations:\n",
    "\n",
    "- **Convert** the collection from their source formats (F0) into a set of tables that conform to the Standard Text Analytic Data Model (F2).\n",
    "- **Annotate** these tables with statistical and linguistic features using NLP libraries such as NLTK (F3).\n",
    "- **Produce** a vector representation of the corpus to generate TFIDF values to add to the TOKEN (aka CORPUS) and VOCAB tables (F4).\n",
    "- **Model** the annotated and vectorized model with tables and features derived from the application of unsupervised methods, including PCA, LDA, and word2vec (F5).\n",
    "- **Explore** your results using statistical and visual methods.\n",
    "- **Present** conclusions about patterns observed in the corpus by means of these operations.\n",
    "\n",
    "When you are finished, you will make the results of your work available in GitHub (for code) and UVA Box (for data). You will submit to Gradescope (via Canvas) a PDF version of a Jupyter notebook that contains the information listed below.\n",
    "\n",
    "# Some Details\n",
    "\n",
    "- Please fill out your answers in each task below by editing the markdown cell. \n",
    "- Replace text that asks you to insert something with the thing, i.e. replace `(INSERT IMAGE HERE)` with an image element, e.g. `![](image.png)`.\n",
    "- For URLs, just paste the raw URL directly into the text area. Don't worry about providing link labels using `[label](link)`.\n",
    "- Please do not alter the structure of the document or cell, i.e. the bulleted lists. \n",
    "- You may add explanatory paragraphs below the bulleted lists.\n",
    "- Please name your tables as they are named in each task below.\n",
    "- Tasks are indicated by headers with point values in parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b6d68-e039-4612-858b-29510eeb5365",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0889de-cd53-4aa5-80b2-a2a39060776a",
   "metadata": {},
   "source": [
    "## Source Description (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e395a-4b0b-4ba3-9112-80c733998dbe",
   "metadata": {},
   "source": [
    "Provide a brief description of your source material, including its provenance and content. Tell us where you found it and what kind of content it contains.\n",
    "\n",
    "This corpus is a digital analytic addition to a recent randomized experiment in teacher education that tested methods for teaching 146 pre-service teachers from two prominent American universities how to metacognitively model unpacking mathematical word problems. The proficiency of the teacher candidates in these areas is measured through short, standardized tasks that require them to record themselves performing the skills. Additionally, teacher candidates recorded themselves performing a metacognitive model in their student-teaching classroom placements. The transcribed recordings from the standardized performance tasks and the classroom observations are the text data used in this project. Labels of interest for the transcripts include treatment status, session (pre, post, or classroom recording), and human rater scores from a 5-item rubric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b507c1-6dc2-44f7-b74c-790d84a48e8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Features (1)\n",
    "\n",
    "Add values for the following items. (Do this for all following bulleted lists.)\n",
    "\n",
    "- Source URL: https://www.dropbox.com/scl/fo/pl8uvwelm5ff8bhp1qvlj/AA0_9fUgMLZ28Y7oloo60dc?rlkey=6unk1pqig6mp4eorrediaxuoc&st=lnqff63o&dl=0\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/pl8uvwelm5ff8bhp1qvlj/AA0_9fUgMLZ28Y7oloo60dc?rlkey=6unk1pqig6mp4eorrediaxuoc&st=lnqff63o&dl=0\n",
    "- Number of raw documents: 1,160\n",
    "- Total size of raw documents (e.g. in MB): 13.6 MB\n",
    "- File format(s), e.g. XML, plaintext, etc.: Word Documents (.docx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e81b1-9f70-47b5-bb25-49be4e76b98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Document Structure (1)\n",
    "\n",
    "Provide a brief description of the internal structure of each document. That, describe the typical elements found in document and their relation to each other. For example, a corpus of letters might be described as having a date, an addressee, a salutation, a set of content paragraphs, and closing. If they are various structures, state that.\n",
    "\n",
    "Each document contains a filename such as 01_S23_001_015_P3, metadata including the time and date that the recording was conducted, a list of keywords (which were discarded), and a body of text with utterances separated by time stamps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4c9f-e101-46fe-ac59-a35a1b148a4b",
   "metadata": {},
   "source": [
    "# Parsed and Annotated Data\n",
    "\n",
    "Parse the raw data into the three core tables of your addition: the `LIB`, `CORPUS`, and `VOCAB` tables.\n",
    "\n",
    "These tables will be stored as CSV files with header rows.\n",
    "\n",
    "You may consider using `|` as a delimitter.\n",
    "\n",
    "Provide the following information for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d05ce4-ac5c-43ea-a07b-c4626338f80e",
   "metadata": {},
   "source": [
    "## LIB (2)\n",
    "\n",
    "The source documents the corpus comprises. These may be books, plays, newspaper articles, abstracts, blog posts, etc. \n",
    "\n",
    "Note that these are *not* documents in the sense used to describe a bag-of-words representation of a text, e.g. chapter.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/01_create_lib_corpus_vocab.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Number of observations: 463\n",
    "- List of features, including at least three that may be used for model summarization (e.g. date, author, etc.): `personid`, `session`, `objective`, `unpacking`, `selfinstruction`, `selfregulation`, `ending`, `accuracy`, `avg_score`, `metadata`, `site`, `course`, `treatment_status`\n",
    "- Average length of each document in characters: 2578.75 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304204a5-00be-46ad-b98b-0d10a9c8ca4b",
   "metadata": {},
   "source": [
    "## CORPUS (2)\n",
    "\n",
    "The sequence of word tokens in the corpus, indexed by their location in the corpus and document structures.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/01_create_lib_corpus_vocab.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Number of observations Between (should be >= 500,000 and <= 2,000,000 observations.): 527,629\n",
    "- OHCO Structure (as delimitted column names): `personid`,`session`,`task`, `sent_num`, `token_num`\n",
    "- Columns (as delimitted column names, including `token_str`, `term_str`, `pos`, and `pos_group`): `token_str`, `term_str`, `pos`, and `pos_group`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3214e-e6dd-42d6-842f-555d0058986e",
   "metadata": {},
   "source": [
    "## VOCAB (2)\n",
    "\n",
    "The unique word types (terms) in the corpus.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/01_create_lib_corpus_vocab.ipynb\n",
    "- Delimitter: \",\" \n",
    "- Number of observations: 7152\n",
    "- Columns (as delimitted names, including `n`, `p`', `i`, `dfidf`, `porter_stem`, `max_pos` and `max_pos_group`, `stop`): `term_str`, `n`, `p`, `i`, `n_chars`, `max_pos_group`, `max_pos`, `n_pos_group`, `cat_pos_group`, `n_pos`, `cat_pos`, `stop`, `porter_stem`, `dfidf`, `mean_tfidf`\n",
    "- Note: Your VOCAB may contain ngrams. If so, add a feature for `ngram_length`.\n",
    "- List the top 20 significant words in the corpus by DFIDF.\n",
    "\n",
    "`put`, `some`, `says`, `looking`, `solve`, `them`, `four`, `total`, `take`, `than`, `10`, `five`, `way`, `an`, `has`, `plus`, `trying`, `she`, `could`, `into`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dabdc-baae-4408-95bc-2f735824d59b",
   "metadata": {},
   "source": [
    "# Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2ef9c-1cb5-41e8-a5ee-1e37428b4539",
   "metadata": {},
   "source": [
    "## BOW (3)\n",
    "\n",
    "A bag-of-words representation of the CORPUS.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/03_derived_tables.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Bag (expressed in terms of OHCO levels): `personid`,`session`,`task`\n",
    "- Number of observations: 151,542 \n",
    "- Columns (as delimitted names, including `n`, `tfidf`): `personid`, `session`, `task`, `term_str`, `n`, `tfidf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29890d2f-bf96-43ad-8d08-792393830163",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DTM (3)\n",
    "\n",
    "A represenation of the BOW as a sparse count matrix.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- UVA Box URL of BOW used to generate (if applicable):\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/03_derived_tables.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Bag (expressed in terms of OHCO levels): `personid`,`session`,`task`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b4774-7c76-401d-a9de-2704f28a0821",
   "metadata": {},
   "source": [
    "## TFIDF (3)\n",
    "\n",
    "A Document-Term matrix with TFIDF values.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- UVA Box URL of DTM or BOW used to create: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/03_derived_tables.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Description of TFIDIF formula ($\\LaTeX$ OK): $w_{i,j} = tf_{i,j} \\times \\log \\frac{N}{df_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f5ca-5361-4701-b9dd-9da66859b40b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reduced and Normalized TFIDF_L2 (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c548dd2-f692-4365-936c-39c84df79b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "A Document-Term matrix with L2 normalized TFIDF values.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- UVA Box URL of source TFIDF table: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/03_derived_tables.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Number of features (i.e. significant words): 474\n",
    "- Principle of significant word selection: I used the top 1000 words from the vocab table with the highest dfidf. I then collapsed the table to a person-by-session table with each cell representing the mean tfidf for each word in each person-by-session document. I then used an L2 normalization (euclidean distances) to get the final distance table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50da94-af36-4e8d-b1a7-24dbcf431880",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df79264-dd93-4199-be38-db31579b7ce8",
   "metadata": {},
   "source": [
    "## PCA Components (4)\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- UVA Box URL of the source TFIDF_L2 table:  https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/04_principal_component_analysis.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Number of components: 10\n",
    "- Library used to generate: https://www.dropbox.com/scl/fi/xzu29vu4tpwnvydv4lfc4/tfidf.csv?rlkey=4lt8oy1lhdvpi15qpu1dudc74&st=b1zi2uqs&dl=0\n",
    "- Top 5 positive terms for first component: `ate`, `tenths`, `pieces`, `more`, `eat`\n",
    "- Top 5 negative terms for second component: `ate`, `tenths`, `eighths`, `brown`, `white`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adc882-cbce-4d24-9923-5d36ac609f43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA DCM (4)\n",
    "\n",
    "The document-component matrix generated.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/04_principal_component\n",
    "- Delimitter: \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd2a4a-7f2f-4259-a5c4-063168cb1b14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA Loadings (4)\n",
    "\n",
    "The component-term matrix generated.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/04_principal_component\n",
    "- Delimitter: \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fff42f-6665-4941-ba3d-034627dc0124",
   "metadata": {},
   "source": [
    "## PCA Visualization 1 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the first two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "![](output/01_figures/pca_visualization1_dcm.png \"Diagram Description\")\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "![](output/01_figures/pca_visualization1_loadings.png \"Diagram Description\")\n",
    "\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the first component:\n",
    "\n",
    "In the plot, the red dots are teacher candidates who scored below the mean, and the blue dots are teachers who scored above the mean.Here, I was interested in determining whether the first two components might provide some insight into why teacher candidates scored higher or lower than the average in each session. The results indicate that the first two components do not provide information about the specific ratings given to teacher candidates. Instead, the polarity (loading strongly on either component 0 or 1) is derived from the standardized prompts used in the performance tasks. Documents that load strongly on the first component feature prompts related to food and fractions, while those loading strongly on the second component involve prompts about money"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb54565-7669-4a2f-90b2-a4c283277c02",
   "metadata": {},
   "source": [
    "## PCA Visualization 2 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the second two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "![](output/01_figures/pca_visualization2_dcm.png \"Diagram Description\")\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "![](output/01_figures/pca_visualization2_loadings.png \"Diagram Description\")\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the second component:\n",
    "\n",
    "In the plot, the red dots are teacher candidates who scored below the mean, and the blue dots are teachers who scored above the mean. The results indicate that the second set of components does provide information about the specific ratings given to teacher candidates. The polarity, which involves loading strongly on either component 2 or 3, stems from some candidates using more 'solving words' (component 2) or more 'process thinking words' (component 3). Teacher candidates were instructed not to solve the problem in their model. Therefore, when candidates tend to use words that load strongly on component 3, such as `times`, `minus`, `place`, `denominator`, `subtract`, they generally score below the average.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee23b2-25d1-4226-bf31-1607e5ed4677",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA TOPIC (4)\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=0fj8ghqy&dl=0\n",
    "- UVA Box URL of count matrix used to create: https://www.dropbox.com/scl/fi/0ejn1vzxe7gl87tw9i5da/CORPUS.csv?rlkey=l0z4m79calnmtz31y0ki5gvd2&st=c165e85k&dl=0 (Corpus is transformed into a count matrix in the jupyter notebook)\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/05_latent_dirichlet_allocation.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Libary used to compute: https://www.dropbox.com/scl/fi/45z0yeg53xbc5y2bz2gf2/LIB.csv?rlkey=0qn09eyr64fq63ogi1e82vyzs&st=m1l5muxy&dl=0\n",
    "- A description of any filtering, e.g. POS (Nouns and Verbs only):`NN`, `NNS`, `JJ`, `JJR`, `JJS`\" Nouns and Adjectives only \n",
    "- Number of components: 20 \n",
    "- Any other parameters used:\n",
    "\n",
    "The topic models were only applied to the classroom observation session transcripts becasue the performance task topics are standardized. The model does a pretty good job of categorizing the main topics that teacher candidates used in their lessons (topics T03-T04)\n",
    "\n",
    "- Top 5 words and best-guess labels for topic five topics by mean document weight:\n",
    "  - T00: `problem`, `number`, `right`, `stickers`, `word`, `line`, `numbers` LABEL: Lessons using visuals such as number lines or stickers\n",
    "  - T01: `problem`, `word`, `number`, `numbers`, `sense`, `right`, `answer`  LABEL: Solving the problem \n",
    "  - T02: `problem`, `minutes`, `right`, `time`, `hours`, `times`, `word` LABEL: Using math problems about time \n",
    "  - T03: `problem`, `numbers`, `right`, `pennies`, `way`, `money`, `tiles` LABEL: Using math problems about money\n",
    "  - T04: `problem`, `candy`, `pieces`, `answer`, `right`, `story`, `equals` LABEL: Using math problems about food and fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518d520-4a5c-48fa-836d-f8ea3e3c2f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA THETA (4)\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/05_latent_dirichlet_allocation.ipynb\n",
    "- Delimitter: \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8808b30-64f4-4249-95d5-d7c0925ce432",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA PHI (4)\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/05_latent_dirichlet_allocation.ipynb\n",
    "- Delimitter: \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e404bf-8a2a-4eb4-ba89-0c708c8f359d",
   "metadata": {},
   "source": [
    "## LDA + PCA Visualization (4)\n",
    "\n",
    "Apply PCA to the PHI table and plot the topics in the space opened by the first two components.\n",
    "\n",
    "Size the points based on the mean document weight of each topic (using the THETA table).\n",
    "\n",
    "Color the points based on a metadata feature from the LIB table.\n",
    "\n",
    "![](output/01_figures/lda_pca_visualization.png \"Diagram Description\")\n",
    "\n",
    "Provide a brief interpretation of what you see.\n",
    "\n",
    "In the plot, the red dots are teacher candidates who were in the control group, and the blue dots are teachers who were in the treatment group. The results indicate that more treatment teachers are associated with topics that have lower loadings on the first component (1). The topics that load the higher on the first component are topic_5, topic_12, topic_19,topic_4, topic_7. Topic_5 is labeled as \"solving the problem,\" which is not part of a good metacognitive model. The result indicates that treatment teachers might be less likely to solve the problem in the classroom. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1f327-a386-476a-8d94-2ab7a63afa7a",
   "metadata": {},
   "source": [
    "## Sentiment VOCAB_SENT (4)\n",
    "\n",
    "Sentiment values associated with a subset of the VOCAB from a curated sentiment lexicon.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- UVA Box URL for source lexicon: https://www.dropbox.com/scl/fo/ry0hzihfvnl13t950uq7a/AHNOLHCIASbw8ayEZGeQPK0?rlkey=zgy42lcabgdjlfbpw18pse35u&st=r7ugsmnl&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/06_sentiment_analysis.ipynb\n",
    "- Delimitter: \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a9d67-1560-4be9-b82a-b99a60b5c93e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment BOW_SENT (4)\n",
    "\n",
    "Sentiment values from VOCAB_SENT mapped onto BOW.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/06_sentiment_analysis.ipynb\n",
    "- Delimitter: \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee6837-b12e-453d-96c1-59eaa4b28883",
   "metadata": {},
   "source": [
    "## Sentiment DOC_SENT (4)\n",
    "\n",
    "Computed sentiment per bag computed from BOW_SENT.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create: https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/06_sentiment_analysis.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Document bag expressed in terms of OHCO levels: `personid`,`session`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cba13-e60a-4940-a06d-02479f002c3c",
   "metadata": {},
   "source": [
    "## Sentiment Plot (4)\n",
    "\n",
    "Plot sentiment over some metric space, such as time.\n",
    "\n",
    "If you don't have a metric metadata features, plot sentiment over a feature of your choice.\n",
    "\n",
    "You may use a bar chart or a line graph.\n",
    "\n",
    "![](output/01_figures/sentiment_plot.png \"Diagram Description\")\n",
    "\n",
    "In general, words associated with trust, joy, and anticipation are used the most in the first session (0). The use of words associated with these emotions declines substantially in the second performance task session (1). The use of these words increases slightly in the classroom session (2) but not to levels seen in the first session (0). The use of words associated with anger, disgust, surprise, fear, and sadness remains low throughout each session. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d2316-317b-4d95-a804-aff98242e411",
   "metadata": {},
   "source": [
    "## VOCAB_W2V (4)\n",
    "\n",
    "A table of word2vec features associated with terms in the VOCAB table.\n",
    "\n",
    "- UVA Box URL: https://www.dropbox.com/scl/fo/mlrsjjrqxinwa33bib5eb/AImfLnlw4KmExgS9ujfWsWk?rlkey=0ggi1vm2hdywrmhyjlny0vb01&st=zuqbwrzu&dl=0\n",
    "- GitHub URL for notebook used to create:  https://github.com/steffenerickson/NSF_Code/blob/9760e5a76495b636982b86a3147da104e71dfa72/04_text_analysis_code/python/07_word_embedding.ipynb\n",
    "- Delimitter: \",\"\n",
    "- Document bag expressed in terms of OHCO levels: `personid` (separate models were run for each session)\n",
    "- Number of features generated:272 for the baseline performance task session (0), 192 for the post-coursework performance task session (1) 227 for the classroom (2)\n",
    "- The library used to generate the embeddings: `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c1974-047b-4285-9f4d-7f3314f39542",
   "metadata": {},
   "source": [
    "## Word2vec tSNE Plot (4)\n",
    "\n",
    "Plot word embedding features in two dimensions using t-SNE.\n",
    "\n",
    "Describe a cluster in the plot that captures your attention.\n",
    "\n",
    "Plot for the classroom session (2)\n",
    "\n",
    "![](output/01_figures/Word2vec_tSNE_Plot.png \"Diagram Description\")\n",
    "\n",
    "The cluster at the top center-left (center around  x = -2 , y = 10) appears to mostly be metacognition words about unpacking the problem. These words include `solve`, `understand`, `doing`, `thinking`, `strategy`, `use`, `problems`, `read`, and  `show`.  These words may be associated higher quality metacognitive models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75878341-7fe8-4e22-b908-36029f9818e8",
   "metadata": {},
   "source": [
    "# Riffs\n",
    "\n",
    "Provde at least three visualizations that combine the preceding model data in interesting ways.\n",
    "\n",
    "These should provide insight into how features in the LIB table are related. \n",
    "\n",
    "The nature of this relationship is left open to you -- it may be correlation, or mutual information, or something less well defined. \n",
    "\n",
    "In doing so, consider the following visualization types:\n",
    "\n",
    "- Hierarchical cluster diagrams\n",
    "- Heatmaps\n",
    "- Scatter plots\n",
    "- KDE plots\n",
    "- Dispersion plots\n",
    "- t-SNE plots\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62acf1-6bb0-45d0-aed2-863b285f8cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 1 (5)\n",
    "\n",
    "![](output/01_figures/riff1.png \"Diagram Description\")\n",
    "\n",
    "This figure is a correlation matrix of human rater scores and the loadings of persons on each component. The strongest correlations between the loadings and the human scores occur between loadings for component 2, and the human scores. Component 2 is associated with solving words such as `times`, `minus`, `place`, `denominator`, and  `subtract`. Solving words are associated with lower scores because teachers were supposed to unpack the math word problem, not solve the problem for students. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155a072-02b3-4aa8-b9f1-e43a59e9a85d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 2 (5)\n",
    "![](output/01_figures/riff2.png \"Diagram Description\")\n",
    "\n",
    "The dendrogram shows the cluster I was referring to in the \"Word2vec tSNE Plot (4)\" section makes up the largest percentage of significant words from the models. These words are mostly process words that describe breaking down the word problems and words associated with directing students to perform tasks or follow along.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067c59b-8983-4acc-972a-1ecd852ded57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 3 (5)\n",
    "\n",
    "![](output/01_figures/riff3.png \"Diagram Description\")\n",
    "\n",
    "The plot shows a moderate negative correlation between words associated with component 2 (solving words) and words associated with high ending the model scores. The negative correlation occurs because  \"ending the model quality\" was scored based on whether teacher candidates solved the problem at the end of the model. If teacher candidates solved the problem at the end of their model, the received a lower score for how they ended the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e25c6e-2624-4899-829e-e7d60c878685",
   "metadata": {},
   "source": [
    "# Interpretation (4)\n",
    "\n",
    "Describe something interesting about your corpus that you discovered during the process of completing this assignment.\n",
    "\n",
    "At a minumum, use 250 words, but you may use more. You may also add images if you'd like.\n",
    "\n",
    "The PCA visualizations show that the thematic content of prompts, such as those related to food and fractions versus money, significantly influences the clustering of documents within the PCA space. This indicates that the content of the prompts substantially dictates the language teacher candidates employ in their responses, which in turn impacts their assessment scores. This needs to be accounted for when scoring teachers on their performances because the content prompts create variation that most likely cannot be attributed to the teachers themselves. The variance suggests that the complexity and nature of mathematical problems can significantly affect teaching strategies. More challenging tasks potentially demand higher cognitive and linguistic engagement from teachers.\n",
    "\n",
    "The analysis also highlighted a small correlation between the use of certain types of instructional language and the candidates' performance. Specifically, the frequent use of 'solving words' is associated with lower performance scores, aligning with instructional guidelines that prioritize problem unpacking over direct problem-solving. On the other hand, process and problem-solving words are associated with higher scores. These results were initially surprising to me. I thought that words such as \"times,\" \"minus,\" \"place,\" \"denominator,\" and \"subtract\" would be correlated with higher human rater scores because they indicate the use of mathematical language. It wasnâ€™t until I realized that they could be associated with solving the problem for students that the negative correlations made sense.\n",
    "\n",
    "The Latent Dirichlet Allocation (LDA) topic modeling provides a view of some of the educational lesson themes used by teacher candidates in the classroom. This analysis identifies topics such as the use of visual aids, problem-solving strategies, and specific mathematical content. Teachers tended to teach lessons on food and fractions, money, and time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
